---
title: "Exogenous and Endogenous Factors that Influence Trust in Visualization"
author: "Jordan Crouser, Syrine Matoussi, and Lan Kung"
date: "2024-02-16"
output:
  html_document: default
  pdf_document: default
---

```{setup, include=FALSE, set.seed(123)}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r}
library(tidyverse)
library(ggplot2)
library(tree)
library(dplyr)
library(rpart.plot)
library(MASS)
library(caret)
library(ztable)
library(GGally)
library(tidyr)
library(kableExtra)
library(randomForest)
library(devtools)
library(reprtree)
select <- dplyr::select
```

# 1. General Cleaning and Statistical Calculations:

```{r}
trust_2023_raw_data <- read_csv("trust_2023_raw_data.csv") %>%
  mutate(response = factor(response, order = TRUE, 
                           levels = c("Strongly Disagree", "Disagree", "Nor", "Agree", "Strongly Agree")),
         likertscale = factor(likertscale, ordered = TRUE),
         category = factor(category, levels = c("I", "N", "G", "S"), 
                           labels = c("Infographic", "News", "Government", "Scientific")),
         education = factor(education, ordered = TRUE, 
                            levels = c("highschool", "associate", "bachelors", "masters", "doctorate"),
                            labels = c("High School", "Associate", "Bachelors", "Masters", "Doctorate")),
degree_names <- c("High School", "Associate", "Bachelor", "Masters", "Doctorate"))
```

## Sources

Previous analysis of this experimental data found that multidimensional trust perception was unevenly distributed between the four high-level categories into which the MASSVis sample images were partitioned: Infographics, News, Government, and Scientific. However, it is important to note that the images in each high-level category were drawn from a relatively narrow selection of sources:

```{r}
trust_2023_raw_data %>%
  select(category, source, image_new) %>%
  unique() %>%
  group_by(category, source) %>%
  summarise(n = n()) %>%
  kbl() %>%
  kable_styling()
```

## Category Charateristics

This narrow range of sources within each category suggests that this dataset may not be representative of the full breadth of each category. Moreover, participants themselves were not privy to each image's category, and so we conclude that bias related to the category itself (e.g. a general mistrust of government media) was not a substantial factor in influencing participants' subjective ratings. Because of this, we begin by further investigating the underlying features that separate these categories to try to gain insight into the specific design features that may be more directly implicated in the behavioral patterns we observe.

First, we will select each image, along with its category and descriptive information (removing duplicate entries, as multiple participants saw each image):

```{r}
category_data <- trust_2023_raw_data %>%
  select(c(image_new, category, vistype, starts_with("attr"))) %>%
  unique() %>%
  mutate(attr_black_white = recode(attr_black_white, y = "B&W", n = "Full Color"),
         attr_human_recognizable_object = recode(attr_human_recognizable_object, y = "Contains a human-recognizable object", n = "No human-recognizable object")) %>%
  rename("Visual Density" = attr_visual_density,
         "Data-Ink Ratio" = attr_data_ink_ratio,
         "# Distinct Colors" = attr_num_distinct_colors)
```


Next, we want to model the sets of features (and interactions between them) that distinguish each category. Because we cannot assume a multivariate normal distribution in our predictors, we opted for a recursive partitioning approach instead of linear discriminant analysis:

```{r}


category_features <- rpart(category ~.-image_new, 
                              data = category_data, 
                              method = "class", 
                              minsplit = 8)

split.fun <- function(x, labs, digits, varlen, faclen)
{
# replace commas with spaces (needed for strwrap)
labs <- gsub(",", " ", labs)
for(i in 1:length(labs)) {
# split labs[i] into multiple lines
labs[i] <- paste(strwrap(labs[i], width = 25), collapse = "\n")
}
labs
}

pdf(file = "tree.pdf", width = 8.5, height = 5)

rpart.plot(category_features, type = 4, 
            clip.right.labs = FALSE, 
            extra=2,  
            #under = TRUE,  
            cex = 0.55,
            fallen.leaves = FALSE,
            branch = 1,
            clip.facs = TRUE,
            #facsep = "\n",
            #split.fun = split.fun,
            ycompress = FALSE,
            Margin = 0.1
           )

dev.off()
```
We use random forest to see what predictors affect accuracy and separation. Human recognizable objects and vistype are the most important feature and this confirms that the tree is not over_fitting.  

```{r}
category_rf <- randomForest(category ~.-image_new, data = category_data, mtry= 8, importance = TRUE) ## mtry number of columns - 1
varImpPlot(category_rf)
```

The confusion matrix for this tree is as follows:

```{r}
category_features_predictions <- predict(category_features, type = "class")

confusionMatrix(category_features_predictions, category_data$category) 
```

Again, we observe that classification accuracy is not uniform across categories

# 2. balanced sampling for strongly agree and disagree:

Because we are interested in the factors that influence trust, and the
directions those influences steer the observerâ€™s ultimate judgment of
the trustworthiness of visualizations rather than absolute measures, we
decided to consider as our dependent variable the deviation from the
average rather than the raw trust scores. To measure the directional push of trust perceptions, we took the average across all
visualizations and then used that as a baseline by which to measure
deviation from the average. This gives us a relative measure, rather
than an absolute one.

We categorize each image in each trust dimension into 3 buckets: "higher trust", "lower trust", and "average trust". We then pick images that across all categories had overall higher or lower trust to see what image attributes influenced that placement. The idea behind this is that if people's trust was highly skewed for a visualization it would be more related to the specific attributes of the image. If an image had an average trust score it means that people likely disagreed about its trustworthiness. That indicates that their choice might have not been only influenced by the attributes of the image but also by their personalities. We therefore run the same analysis with the participant ID as the predictor instead of the image ID.

# a. Seperating images based on deviation from the average trust:

To separate images by the level of overall trust they received we start by changing "response" from a categorical to a quantitative variable. We then calculate the average response score of each image and the overall average and standard deviation of all image responses. An agreement average is higher or lower if it is more than one sd from the overall mean. The formula resulted from experimenting with various coefficients for the standard deviation. We determined that a higher coefficient (2sd or 3sd) produced an excessively small and inconclusive set of images, while a lower coefficient led to the selection of an excessively large set of images. Finally, we calculate the general response as follows:

- Higher general agreement is when at least two dimensions have higher trust and the rest are averages.

- Lower general agreement is when at least two dimensions have lower trust and the rest are averages.

- Mixed general agreement is when we have both lower and higher responses for one image.

In the analysis below, we separate images that have generally higher or lower trust across all five dimensions, to see which of the attributes contribute to that placement of trust. 

```{r}
image_trust_placement <- trust_2023_raw_data %>%
  group_by(name, image_new, response) %>%
  summarise(n = n()) %>%
  arrange(name, response) %>%
  pivot_wider(names_from = response, values_from = n, values_fill = 0) %>%
  mutate(
    total_responses = `Strongly Disagree` + `Disagree` + `Nor` + `Agree` + `Strongly Agree`,
    agreement_mean = (2 * `Strongly Agree` + 1 * `Agree` + 0 * `Nor` - 1 * `Disagree` - 2 * `Strongly Disagree`) / total_responses
  ) %>%
  ungroup() %>%
  group_by(name) %>%
  mutate(
    overall_mean = mean(agreement_mean, na.rm = TRUE),
    overall_sd = sd(agreement_mean, na.rm = TRUE),
    agreement_level = case_when(
      agreement_mean > overall_mean + overall_sd ~ "higher",
      agreement_mean < overall_mean - overall_sd ~ "lower",
      TRUE ~ "avg"
    )
  ) %>%
  select(name, image_new, agreement_level) %>%
  mutate(
    name = case_when(
      name == "I am familiar with the topic or data this visualization presents." ~ "Familiarity",
      name == "I believe the visualization shows real data." ~ "Credibility",
      name == "I understand what this visualization is trying to tell me." ~ "Clarity",
      name == "I would rely on the facts in this Visualization." ~ "Reliability",
      name == "I would feel confident using the information to make a decision." ~ "Confidence",
      TRUE ~ name
    )
  ) %>%
  pivot_wider(names_from = name, values_from = agreement_level) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(
    General = case_when(
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "lower") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("lower", "avg")) == 5 ~ "lower",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "higher") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("higher", "avg")) == 5 ~ "higher",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "avg") >= 4 ~ "avg",
      TRUE ~ "mixed"
    )
  ) %>%
  select(image_new, General, Familiarity, Credibility, Clarity, Confidence, Reliability)
```

We notice a lack of consistent patterns where trust dimensions exhibit opposing trends. The trust dimensions tend to move collectively, either increasing or decreasing. When one dimension is higher, others are divided between 'average' and 'higher.' Conversely, if one dimension is lower, the remaining dimensions tend to be split between 'average' and 'lower' Exceptions exist where 2 to 3 dimensions may be higher, one lower, and the rest average, or vice versa.

We look at the distribution of responses to make sure that higher and lower trust averages have significant percentages and are greater than mixed.

```{r}
image_trust_placement_percentages <- image_trust_placement %>%
  group_by(General) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(percentage = count / sum(count) * 100)
image_trust_placement_percentages

```

We merge our image_trust_placement dataset with the raw one to add the image attribute columns.

# b. Visualizing the influence of visualization's attributes on trust:

```{r}
merged_image_trust_placement <- trust_2023_raw_data %>%
  left_join(image_trust_placement, by = "image_new") %>%
  group_by(image_new) %>%
    select(image_new, vistype, General, category, `attr_data_ink_ratio`,
         `attr_num_distinct_colors`, `attr_black_white`, 
         `attr_visual_density`, `attr_human_recognizable_object`, 
         `attr_human_depiction`)%>%
    filter(General %in% c("higher", "lower"))%>%


   slice(1)  
merged_image_trust_placement%>%
unique()


```

We finally create a decision tree using the image attributes as predictors to see how they affect trust directions.

```{r}
vis_attr_prelim_tree <- rpart(General ~.-image_new -category, data = merged_image_trust_placement, method="class")
```

```{r}
pdf(file = "vis_attr_prelim_tree.pdf", width = 8.5, height = 5)

rpart.plot(vis_attr_prelim_tree, type = 4, 
            clip.right.labs = FALSE, 
            extra=2,  
            #under = TRUE,  
            cex = 0.55,
            fallen.leaves = FALSE,
            branch = 1,
            clip.facs = TRUE,
            #facsep = "\n",
            #split.fun = split.fun,
            ycompress = FALSE,
            Margin = 0.1
           )

dev.off()
```

We find that areas, bars, circles, and maps tend to have higher trust than diagrams, tables, trees, and grids. We also find that visualizations with human recognizable objects tend to be more trusted by people. On the third level of the decision tree, we find that diagrams and points tend to have higher trust than lines, tables, trees, and networks.

```{r}
merged_image_trust_placement$General = factor(merged_image_trust_placement$General) 
general_image_rf <- randomForest(General ~.-image_new -category, data = merged_image_trust_placement, mtry= 7, importance = TRUE) 
varImpPlot(general_image_rf)

```

We use random forests to see what predictors affect accuracy and separation. Human recognizable objects and vistype are the most important features and this confirms that the tree is not overfitting.

# Seperate individuals based on deviation from the average trust:

In the chunk below we assess the mean agreement and standard deviation for each individual.

```{r}
individual_averages <- trust_2023_raw_data %>%
  mutate(
    response_numerical = case_when(
      response == "Strongly Disagree" ~ -2,
      response == "Disagree" ~ -1,
      response == "Nor" ~ 0,
      response == "Agree" ~ 1,
      response == "Strongly Agree" ~ 2,
      TRUE ~ NA_real_
    )
  ) %>%
  group_by(name, session_id) %>%
  summarise(
    agreement_mean = mean(response_numerical, na.rm = TRUE),
    agreement_sd = sd(response_numerical, na.rm = TRUE)
  ) %>%
  mutate(
    name = case_when(
      name == "I am familiar with the topic or data this visualization presents." ~ "Familiarity",
      name == "I believe the visualization shows real data." ~ "Credibility",
      name == "I understand what this visualization is trying to tell me." ~ "Clarity",
      name == "I would rely on the facts in this Visualization." ~ "Reliability",
      name == "I would feel confident using the information to make a decision." ~ "Confidence",
      TRUE ~ name
    )
  )
```

We then display it side by side to search for patterns.

```{r}
individual_averages_table<- individual_averages %>%
  pivot_wider(names_from = name, values_from = c(agreement_mean, agreement_sd))%>%
  mutate(
    Familiarity = paste(round(agreement_mean_Familiarity, 2), "/", round(agreement_sd_Familiarity, 2)),
    Credibility = paste(round(agreement_mean_Credibility, 2), "/", round(agreement_sd_Credibility, 2)),
    Clarity = paste(round(agreement_mean_Clarity, 2), "/", round(agreement_sd_Clarity, 2)),
    Reliability = paste(round(agreement_mean_Reliability, 2), "/", round(agreement_sd_Reliability, 2)),
    Confidence = paste(round(agreement_mean_Confidence, 2), "/", round(agreement_sd_Confidence, 2))
  ) %>%
  select(session_id, Familiarity, Credibility, Clarity, Confidence, Reliability)
```

We repeat the same analysis in part 1 but we group by individuals instead of images. In our merge we select columns that contain individual traits.

```{r}
individual_placement <- individual_averages %>%
  ungroup() %>%
  group_by(name) %>%
  mutate(
    overall_mean = mean(agreement_mean, na.rm = TRUE),
    overall_sd = sd(agreement_mean, na.rm = TRUE),
    agreement_level = case_when(
      agreement_mean > overall_mean + overall_sd ~ "higher",
      agreement_mean < overall_mean - overall_sd ~ "lower",
      TRUE ~ "avg"
    )
  ) %>%
  select(name, session_id, agreement_level) %>%
  pivot_wider(names_from = name, values_from = agreement_level) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(
    General = case_when(
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "lower") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("lower", "avg")) == 5 ~ "lower",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "higher") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("higher", "avg")) == 5 ~ "higher",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "avg") >= 4 ~ "avg",
      TRUE ~ "mixed"
    )
  ) %>%
  select(session_id, General, Familiarity, Credibility, Clarity, Confidence, Reliability)

```

```{r}
merged_individual <- trust_2023_raw_data %>%
  left_join(individual_placement, by = "session_id") %>%
  group_by(session_id) %>%
  select(session_id, General, age, sex, education, mini_score)%>%
  filter(General %in% c("higher", "lower"))%>%

  slice(1)  
merged_individual

```
# visualizing the influence of individual differences on trust:

```{r}
individual_placement_percentages <- individual_placement %>%
  group_by(General) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(percentage = count / sum(count) * 100)

individual_placement_percentages

```

```{r, rotate=90}
individual_char_prelim_tree <- rpart(General ~.-session_id -sex -age, data = merged_individual, method="class", minbucket = 10)
rpart.plot(individual_char_prelim_tree, 
           type = 4, 
           clip.right.labs = FALSE, 
           extra=2,  
           #under = TRUE,  
           cex = 0.55,
           fallen.leaves = FALSE,
           branch = 1,
           clip.facs = TRUE
)
```


From the decision tree, we find that data literacy (mini-score) is the biggest predictor for individual trust followed by education. In fact, people with a data visualization score over 6 tend to have higher trust than those with a score under 6.

```{r}
merged_individual$General = factor(merged_individual$General) 
general_individual_rf <- randomForest(General ~.-session_id, data = merged_individual, mtry= 5, importance = TRUE) 
varImpPlot(general_individual_rf)

```

Mini_score is also important for both accuracy and purity.

In the below chunk, we  visualize how higher and lower trust is distributed by visual literacy, sex, and education to see if these 3 variables are correlated or if one is proxy for the others. We find that they are independent. 

```{r}
education_order <- c("High School", "Associate", "Bachelors", "Masters", "Doctorate")

merged_individual %>%
  mutate(education = case_match(education, 
                                "highschool" ~ "High School",
                                "associate" ~ "Associate", 
                                "bachelors" ~ "Bachelors", 
                                "masters" ~ "Masters", 
                                "doctorate" ~ "Doctorate"),
         education = factor(education, education_order)) %>%
ggplot(aes(x=age,y=mini_score, fill=sex))+geom_point(color="black", pch=21, size=2)+facet_grid(~education) +
  ylab("Visualization Literacy Score\n(MiniVLAT)") +
  xlab("Age (years)") + 
  scale_y_continuous(breaks=seq(0,12,2)) +
  scale_fill_manual(name = "Sex (self-reported)", labels = c("Female", "Male"), values=c("darkorchid","deepskyblue"))+
  guides(fill="legend") + theme(legend.position="bottom")
        



```

```{r}
merged_individual%>%
  mutate(over_59= age>=59 )%>%
  group_by(sex, over_59,education)%>%
  summarize(n=n())%>%
  pivot_wider(names_from= education, values_from = n)
```

Next we wanted to investigate stimuli causing the wide range of responses for every visualization so we group by image and we count the total of each response (Strongly Disagree, Disagree, Nor, Agree, Strongly Agree) 

## Are there some stimuli that got a wide range or responses?
```{r}
trust_2023_raw_data %>%
  filter(name == "I believe the visualization shows real data.") %>%
  group_by(image_new, response) %>%
  summarise(n= n()) %>%
  arrange(response) %>%
  pivot_wider(names_from = response, values_from = n, values_fill = 0) %>%
  rowwise() %>%
  mutate(total_uses = sum(`Strongly Disagree`, `Disagree`, `Nor`, `Agree`, `Strongly Agree`)) %>%
  filter(total_uses > 1) %>%
  arrange(desc(total_uses))
```
We then build a model predicting responses for the belief dimension and create a table to compare the predicted response and real response. We find that the model tends to have more errors around the extremities (strongly agree and disagree).

```{r}
believe_data <- trust_2023_raw_data %>%
  filter(name == "I believe the visualization shows real data.") %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
believe_prelim_tree <- rpart(response ~.-image_new , data = believe_data, method="class", minsplit = 4)
believe_pred = predict(believe_prelim_tree, believe_data, type = "class")

table(believe_pred, believe_data$response)
```
To explore the reason why some images have more errors we group by images and responses, convert responses to a numerical variable and count images that had the biggest difference between the predicted and true response. We then make a collage to look for visual trends that influenced the error. We copy the image links from the table to find the originals at the following link:  https://people.csail.mit.edu/zoya/VisThumbnails/fullsize/. The collage can be found at : https://docs.google.com/document/d/1EW2BTxkq9Z1sNG2keglcOrogPAOZC7uPksYKxCZFWOg/edit?usp=sharing


## Predicted vs. true
```{r}
believe_data_with_pred <- cbind(believe_data, pred = believe_pred) %>%
  mutate(pred = factor(pred, order = TRUE, 
                           levels = c("Strongly Disagree", "Disagree", "Nor", "Agree", "Strongly Agree")))
```  

## Are there some stimuli that got a wide range or responses?
```{r}
believe_data_with_pred %>%
  group_by(image_new, response) %>%
  summarise(n= n()) %>%
  arrange(response) %>%
  pivot_wider(names_from = response, values_from = n, values_fill = 0) %>%
  rowwise() %>%
  mutate(total_uses = sum(`Strongly Disagree`, `Disagree`, `Nor`, `Agree`, `Strongly Agree`)) %>%
  filter(total_uses > 1) %>%
  arrange(desc(total_uses))
```

## Images with largest errors
```{r}
believe_data_with_pred %>%
  group_by(image_new, response) %>%
  summarise(n= n(),
            pred = pred) %>%
  mutate(error = as.numeric(response) - as.numeric(pred)) %>%
  filter(abs(error) > 2) %>%
  arrange(image_new, desc(n))
```
We notice from the collage that most of the mislabeled visualizations are horizontal bar charts, tables, and infographics. 

# 3. Anaylyze trust for every dimension: 

After analyzing trust responses for individuals and images for all dimensions, we build decision trees to see the main predictors for each dimension of trust. For that we filter by extreme agreement and disagreement responses and trust dimension. Given the substantial skew towards strong agreement in responses, we deliberately chose a sample of 125 images from each agreement level to balance our dataset. In order to reduce run time we discard irrelevant columns (that are not related to image attributes or individual characteristics). We then run our decision tree based on the wrangled dataset.

## Belief:

```{r}
strongly_believe_data <- trust_2023_raw_data %>%
  filter(name == "I believe the visualization shows real data." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
strong_belief_prelim_tree <- rpart(response ~.-image_new -age -sex, data = strongly_believe_data, method="class")
```

```{r}
rpart.plot(strong_belief_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

We find that the significant determinants of credibility are: vis type, colorfulness, and data literacy

```{r}
strongly_believe_data$response = factor(strongly_believe_data$response) 
belief_rf <- randomForest(response ~.-image_new -age -sex, data = strongly_believe_data, mtry= 10, importance = TRUE) 
varImpPlot(belief_rf)
```
We use random forest to see what predictors affect accuracy and purity. 
- human depiction and education are the most important features for accuracy
- Data literacy and vistype are the most important features for purity




##Clarity:

```{r}
strongly_clear_data <- trust_2023_raw_data %>%
  filter(name == "I understand what this visualization is trying to tell me." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
strong_clarity_prelim_tree <- rpart(response ~.-image_new -sex -age, data = strongly_clear_data, method="class")
```

```{r}
rpart.plot(strong_clarity_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```
The significant determinants of clarity are data literacy, human recognizable objects, and vis type

```{r}
strongly_clear_data$response = factor(strongly_clear_data$response) 
clarity_rf <- randomForest(response ~.-image_new -sex -age, data = strongly_clear_data, mtry= 10, importance = TRUE) 
varImpPlot(clarity_rf)
```
For clarity:
- mini score and human recognizable objects are the most important features for accuracy
- mini score  and vistype are the most important features for purity



##Reliability:

```{r}
strongly_reliable_data <- trust_2023_raw_data %>%
  filter(name == "I would rely on the facts in this Visualization." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source)) 
```

```{r}
strong_reliability_prelim_tree <- rpart(response ~.-image_new -sex -age , data = strongly_reliable_data, method="class")
```

```{r}
rpart.plot(strong_reliability_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

The significant determinants of reliability are vis type and colorefulness

```{r}
strongly_reliable_data$response = factor(strongly_reliable_data$response) 
reliability_rf <- randomForest(response ~.-image_new -sex -age, data = strongly_reliable_data, mtry= 10, importance = TRUE) 
varImpPlot(reliability_rf)
```
For reliability:
- data ink ratio and black and white attributes that are the most important features for accuracy
- mini score  and vistype are the most important features for purity

##Familiarity

```{r}
strongly_familiar_data <- trust_2023_raw_data %>%
  filter(name == "I am familiar with the topic or data this visualization presents." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale)%>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
strong_familiar_prelim_tree <- rpart(response ~.-image_new -sex -age , data = strongly_familiar_data, method="class")
```

```{r}
rpart.plot(strong_familiar_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

The significant determinants of familiarity are vistype and data literacy

```{r}
strongly_familiar_data$response = factor(strongly_familiar_data$response) 
familiarity_rf <- randomForest(response ~.-image_new -sex -age, data = strongly_familiar_data, mtry= 10, importance = TRUE) 
varImpPlot(familiarity_rf)
```
For familiarity:
- black and white attributes and human depiction are the most important features for accuracy
- mini score  and vistype are the most important features for purity

##Confidence:

```{r}
strong_confidence_data <- trust_2023_raw_data %>%
  filter(name == "I would feel confident using the information to make a decision." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```


```{r}
strong_confidence_prelim_tree <- rpart(response ~.-image_new -sex -age, data = strong_confidence_data, method="class")
```

```{r}
rpart.plot(strong_confidence_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

The significant determinants of confidence are vis type and miniscore.

```{r}
strong_confidence_data$response = factor(strong_confidence_data$response) 
confidence_rf <- randomForest(response ~.-image_new -sex -age, data = strong_confidence_data, mtry= 10, importance = TRUE) 
varImpPlot(confidence_rf)
```
For confidence:
- black and white attributes and human depiction are the most important features for accuracy
- mini score  and vistype are the most important features for purity

From analyzing each of the 5 dimensions of trust's decision trees we conclude that the most common determinants of trust in data visualizations are:

- Visualization type: people tend to trust bars and circles more than other types
- Data Literacy: people with higher data literacy usually tend to trust visualizations more
- Colorfulness: this variable fluctuates but usually more colorful visualizations are more trustworthy
- Education: higher education is usually correlated with higher trust





