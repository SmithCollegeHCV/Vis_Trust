---
title: "Trust Analysis 2024"
author: "Jordan"
date: "2023-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggplot2)
library(tree)
library(dplyr)
library(rpart.plot)
library(MASS)
library(caret)
library(ztable)
library(GGally)
library(tidyr)
library(kableExtra)
library(randomForest)
select <- dplyr::select
```

# 1. General Cleaning and Statistical Calculations:

```{r}
trust_2023_raw_data <- read_csv("trust_2023_raw_data.csv") %>%
  mutate(response = factor(response, order = TRUE, 
                           levels = c("Strongly Disagree", "Disagree", "Nor", "Agree", "Strongly Agree")),
         likertscale = factor(likertscale, ordered = TRUE),
         category = factor(category, levels = c("I", "N", "G", "S"), 
                           labels = c("Infographic", "News", "Government", "Scientific")))
```

## Sources

Previous analysis of this experimental data found that multidimensional trust perception was unevenly distributed between the four high-level categories into which the MASSVis sample images were partitioned: Infographics, News, Government, and Scientific. However, it is important to note that the images in each high-level category were drawn from a relatively narrow selection of sources:

```{r}
trust_2023_raw_data %>%
  select(category, source, image_new) %>%
  unique() %>%
  group_by(category, source) %>%
  summarise(n = n()) %>%
  kbl() %>%
  kable_styling()
```

## Category Charateristics

This narrow range of sources within each category suggests that this dataset may not be representative of the full breadth of each category. Moreover, participants themselves were not privy to each image's category, and so we conclude that bias related to the category itself (e.g. a general mistrust of government media) was not a substantial factor in influencing participants' subjective ratings. Because of this, we begin by further investigating the underlying features that separate these categories to try to gain insight into the specific design features that may be more directly implicated in the behavioral patterns we observe.

First, we will select each image, along with its category and descriptive information (removing duplicate entries, as multiple participants saw each image):

```{r}
category_data <- trust_2023_raw_data %>%
  select(c(image_new, category, vistype, starts_with("attr"))) %>%
  unique()
```


Next, we want to model the sets of features (and interactions between them) that distinguish each category. Because we cannot assume a multivariate normal distribution in our predictors, we opted for a recursive partitioning approach instead of linear discriminant analysis:

```{r}
category_features <- rpart(category ~.-image_new, 
                              data = category_data, 
                              method = "class", 
                              minsplit = 8)

rpart.plot(category_features, type = 4, clip.right.labs = FALSE, extra=2)
```
We use random forest to see what predictors affect accuracy and separation. Human recognizable objects and vistype are the most important feature and this confirms that the tree is not over_fitting. 

```{r}
category_rf <- randomForest(category ~.-image_new, data = category_data, mtry= 8, importance = TRUE) ## mtry number of columns - 1
varImpPlot(category_rf)
```

The confusion matrix for this tree is as follows:
```{r}
category_features_predictions <- predict(category_features, type = "class")

confusionMatrix(category_features_predictions, category_data$category) 
```

Again, we observe that classification accuracy is not uniform across categories

# 2. balanced sampling for strongly agree and disagree:

Next we wanted to pinpoint the predictors relating to the visualization attributes and individual differences that tend to influence trust in certain directions. To achieve that we categorize each images in each trust dimension into 3 buckets: "higher trust", "lower trust", and "average trust". We then pick images that across all categories had overall higher or lower trust to see what image attributes influenced that placement. The idea behind this being that if people were generally skewed to trust a visualization or not it would be related to the specific attributes of the image rather than other factors. If an images had an average overall trust it means that people had disagreement in its trustworthiness. That indicates that their choice was not just influenced by the attributes of the image but also their own personalities. We therefore run the same analysis on individuals instead of the images.

# a. seperate images based on deviation from the average trust:

To separate images by the level of overall trust they received we start by changing "response" from a categorical to quantitative variable. We then calculate the average response score of each image and the overall average and standard deviation of all image responses. An agreement avg is higher or lower if it is more than one sd from the overall mean. The formula resulted from experimenting with various coefficients for the standard deviation. We determined that a higher coefficient (2sd or 3sd) produced an excessively small and inconclusive set of images, while a lower coefficient led to the selection of an excessively large set of images. Finally we calculate the general response as follow:
- higher general agreement is when at least two dimensions have higher trust and the rest are averages
- lower general agreement is when at least two dimensions have lower trust and the rest are averages
- mixed general agreement is when we have both lower and higher responses for one image

```{r}
strongly_believe_data <- trust_2023_raw_data %>%
  filter(name == "I believe the visualization shows real data." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```


```{r}
strong_belief_prelim_tree <- rpart(response ~.-image_new , data = strongly_believe_data, method="class")
```

```{r}
rpart.plot(strong_belief_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

```{r}
response_rf <- randomForest(response ~.-image_new, data = strongly_believe_data, mtry= 12, importance = TRUE) ## mtry number of columns - 1
varImpPlot(response_rf)
```

##Clarity:

```{r}
strongly_clear_data <- trust_2023_raw_data %>%
  filter(name == "I understand what this visualization is trying to tell me." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```


```{r}
strong_clarity_prelim_tree <- rpart(response ~.-image_new, data = strongly_clear_data, method="class")
```

```{r}
rpart.plot(strong_clarity_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

##Reliability:

```{r}
strongly_reliable_data <- trust_2023_raw_data %>%
  filter(name == "I would rely on the facts in this Visualization." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source)) 
```


```{r}
strong_reliability_prelim_tree <- rpart(response ~.-image_new , data = strongly_reliable_data, method="class")
```

```{r}
rpart.plot(strong_reliability_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

##Familiarity

```{r}
strongly_familiar_data <- trust_2023_raw_data %>%
  filter(name == "I am familiar with the topic or data this visualization presents." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale)%>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```


```{r}
strong_familiar_prelim_tree <- rpart(response ~.-image_new , data = strongly_familiar_data, method="class")
```

```{r}
rpart.plot(strong_familiar_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

##Confidence:


```{r}
strong_confidence_data <- trust_2023_raw_data %>%
  filter(name == "I would feel confident using the information to make a decision." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```


```{r}
strong_confidence_prelim_tree <- rpart(response ~.-image_new , data = strong_confidence_data, method="class")
```

```{r}
rpart.plot(strong_confidence_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```
# 3. Find General Higher or Lower Trust for Images and Individuals 

# seperate images based on deviation from the usual:

In the analysis below, we separate images based that have general higher or lower trust across all five dimensions, to see which of the attributes contribute to that placement of trust. 

```{r}
image_trust_placement <- trust_2023_raw_data %>%
  group_by(name, image_new, response) %>%
  summarise(n = n()) %>%
  arrange(name, response) %>%
  pivot_wider(names_from = response, values_from = n, values_fill = 0) %>%
  mutate(
    total_responses = `Strongly Disagree` + `Disagree` + `Nor` + `Agree` + `Strongly Agree`,
    agreement_mean = (2 * `Strongly Agree` + 1 * `Agree` + 0 * `Nor` - 1 * `Disagree` - 2 * `Strongly Disagree`) / total_responses
  ) %>%
  ungroup() %>%
  group_by(name) %>%
  mutate(
    overall_mean = mean(agreement_mean, na.rm = TRUE),
    overall_sd = sd(agreement_mean, na.rm = TRUE),
    agreement_level = case_when(
      agreement_mean > overall_mean + overall_sd ~ "higher",
      agreement_mean < overall_mean - overall_sd ~ "lower",
      TRUE ~ "avg"
    )
  ) %>%
  select(name, image_new, agreement_level) %>%
  mutate(
    name = case_when(
      name == "I am familiar with the topic or data this visualization presents." ~ "Familiarity",
      name == "I believe the visualization shows real data." ~ "Credibility",
      name == "I understand what this visualization is trying to tell me." ~ "Clarity",
      name == "I would rely on the facts in this Visualization." ~ "Reliability",
      name == "I would feel confident using the information to make a decision." ~ "Confidence",
      TRUE ~ name
    )
  ) %>%
  pivot_wider(names_from = name, values_from = agreement_level) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(
    General = case_when(
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "lower") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("lower", "avg")) == 5 ~ "lower",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "higher") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("higher", "avg")) == 5 ~ "higher",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "avg") >= 4 ~ "avg",
      TRUE ~ "mixed"
    )
  ) %>%
  select(image_new, General, Familiarity, Credibility, Clarity, Confidence, Reliability)
```

We notice a lack of consistent patterns where trust dimensions exhibit opposing trends. The trust dimensions tend to move collectively, either increasing or decreasing. When one dimension is higher, others are divided between 'average' and 'higher.' Conversely, if one dimension is lower, the remaining dimensions tend to be split between 'average' and 'lower' Exceptions exist where 2 to 3 dimensions may be higher, one lower, and the rest average, or vice versa.

We look at the distribution of responses to make sure that higher and lower have significant percentages and are greater than mixed

```{r}
image_trust_placement_percentages <- image_trust_placement %>%
  group_by(General) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(percentage = count / sum(count) * 100)
image_trust_placement_percentages

```

We merge our image_trust_placement dataset with the raw one to select and add the image attributes columns

# b. Visualizing the influence of visualization's attributes on trust:

```{r}
merged_image_trust_placement <- trust_2023_raw_data %>%
  left_join(image_trust_placement, by = "image_new") %>%
  group_by(image_new) %>%
    select(image_new, vistype, General, category, `attr: data-ink ratio`,
         `attr: # distinct colors`, `attr: black&white`, 
         `attr: visual density`, `attr: human recognizable object`, 
         `attr: human depiction`)%>%

   slice(1)  
merged_image_trust_placement


```
We finally use the image attributes as predictors in our decision tree to see how they affect trust.

```{r}
vis_attr_prelim_tree <- rpart(General ~.-image_new -category, data = merged_image_trust_placement, method="class", minsplit = 8)
```

```{r}
rpart.plot(vis_attr_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```
We find that visualization type and human recognizable objects are the biggest predictors of trust.

# Seperate individuals based on deviation from the average trust:

In the chunk below we asses the mean agreement and sd for each individual

```{r}
individual_averages <- trust_2023_raw_data %>%
  mutate(
    response_numerical = case_when(
      response == "Strongly Disagree" ~ -2,
      response == "Disagree" ~ -1,
      response == "Nor" ~ 0,
      response == "Agree" ~ 1,
      response == "Strongly Agree" ~ 2,
      TRUE ~ NA_real_
    )
  ) %>%
  group_by(name, session_id) %>%
  summarise(
    agreement_mean = mean(response_numerical, na.rm = TRUE),
    agreement_sd = sd(response_numerical, na.rm = TRUE)
  ) %>%
  mutate(
    name = case_when(
      name == "I am familiar with the topic or data this visualization presents." ~ "Familiarity",
      name == "I believe the visualization shows real data." ~ "Credibility",
      name == "I understand what this visualization is trying to tell me." ~ "Clarity",
      name == "I would rely on the facts in this Visualization." ~ "Reliability",
      name == "I would feel confident using the information to make a decision." ~ "Confidence",
      TRUE ~ name
    )
  )
```

We then display it side by side to search for patterns.

```{r}
individual_averages_table<- individual_averages %>%
  pivot_wider(names_from = name, values_from = c(agreement_mean, agreement_sd))%>%
  mutate(
    Familiarity = paste(round(agreement_mean_Familiarity, 2), "/", round(agreement_sd_Familiarity, 2)),
    Credibility = paste(round(agreement_mean_Credibility, 2), "/", round(agreement_sd_Credibility, 2)),
    Clarity = paste(round(agreement_mean_Clarity, 2), "/", round(agreement_sd_Clarity, 2)),
    Reliability = paste(round(agreement_mean_Reliability, 2), "/", round(agreement_sd_Reliability, 2)),
    Confidence = paste(round(agreement_mean_Confidence, 2), "/", round(agreement_sd_Confidence, 2))
  ) %>%
  select(session_id, Familiarity, Credibility, Clarity, Confidence, Reliability)
```

We repeat the same analysis in part a but we group by individuals instead of images. In our merge we select columns that contain individual traits.

```{r}
individual_placement <- individual_averages %>%
  ungroup() %>%
  group_by(name) %>%
  mutate(
    overall_mean = mean(agreement_mean, na.rm = TRUE),
    overall_sd = sd(agreement_mean, na.rm = TRUE),
    agreement_level = case_when(
      agreement_mean > overall_mean + overall_sd ~ "higher",
      agreement_mean < overall_mean - overall_sd ~ "lower",
      TRUE ~ "avg"
    )
  ) %>%
  select(name, session_id, agreement_level) %>%
  pivot_wider(names_from = name, values_from = agreement_level) %>%
  ungroup() %>%
  rowwise() %>%
  mutate(
    General = case_when(
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "lower") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("lower", "avg")) == 5 ~ "lower",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "higher") >= 2 &
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) %in% c("higher", "avg")) == 5 ~ "higher",
      sum(c(Familiarity, Credibility, Clarity, Confidence, Reliability) == "avg") >= 4 ~ "avg",
      TRUE ~ "mixed"
    )
  ) %>%
  select(session_id, General, Familiarity, Credibility, Clarity, Confidence, Reliability)

```

```{r}
merged_individual <- trust_2023_raw_data %>%
  left_join(individual_placement, by = "session_id") %>%
  group_by(session_id) %>%
  select(session_id, General, age, sex, education, mini_score)%>%
  filter(General %in% c("higher", "lower"))%>%

  slice(1)  
merged_individual

```
# visualizing the influence of individual differences on trust:

```{r}
individual_placement_percentages <- individual_placement %>%
  group_by(General) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(percentage = count / sum(count) * 100)

individual_placement_percentages

```


```{r}
individual_char_prelim_tree <- rpart(General ~.-session_id, data = merged_individual, method="class", minsplit=4)
```

```{r}
rpart.plot(individual_char_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```
From the decision tree we find that age and literacy score are the biggest predictors for individual trust.

In the below chunk, we  visualize how higher and lower trust is distributed by visual literacy, sex, and education to see if these 3 variables are correlated or if one is proxy for the others. We find that they are independent. 

```{r}
education_order <- c("highschool", "associate", "bachelors", "masters", "doctorate")

merged_individual <- merged_individual %>%
  mutate(education = factor(education, levels = education_order))
merged_individual%>%
ggplot(aes(x=age,y=mini_score, color=General))+geom_point()+facet_grid(sex~education)
```

```{r}
merged_individual%>%
  mutate(over_59= age>=59 )%>%
  group_by(sex, over_59,education)%>%
  summarize(n=n())%>%
  pivot_wider(names_from= education, values_from = n)
```

Next we wanted to investigate stimuli causing the wide range of responses for every visualization so we group by image and we count the total of each response (Strongly Disagree, Disagree, Nor, Agree, Strongly Agree) 

## Are there some stimuli that got a wide range or responses?
```{r}
trust_2023_raw_data %>%
  filter(name == "I believe the visualization shows real data.") %>%
  group_by(image_new, response) %>%
  summarise(n= n()) %>%
  arrange(response) %>%
  pivot_wider(names_from = response, values_from = n, values_fill = 0) %>%
  rowwise() %>%
  mutate(total_uses = sum(`Strongly Disagree`, `Disagree`, `Nor`, `Agree`, `Strongly Agree`)) %>%
  filter(total_uses > 1) %>%
  arrange(desc(total_uses))
```
We then build a model predicting responses for the belief dimension and create a table to compare the predicted response and real response. We find that the model tends to have more errors around the extremities (strongly agree and disagree).

```{r}
believe_data <- trust_2023_raw_data %>%
  filter(name == "I believe the visualization shows real data.") %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
believe_prelim_tree <- rpart(response ~.-image_new , data = believe_data, method="class", minsplit = 4)
believe_pred = predict(believe_prelim_tree, believe_data, type = "class")

table(believe_pred, believe_data$response)
```
To explore the reason why some images have more errors we group by images and responses, convert responses to a numerical variable and count images that had the biggest difference between the predicted and true response. We then make a collage to look for visual trends that influenced the error. We copy the image links from the table to find the originals at the following link:  https://people.csail.mit.edu/zoya/VisThumbnails/fullsize/. The collage can be found at : https://docs.google.com/document/d/1EW2BTxkq9Z1sNG2keglcOrogPAOZC7uPksYKxCZFWOg/edit?usp=sharing


## Predicted vs. true
```{r}
believe_data_with_pred <- cbind(believe_data, pred = believe_pred) %>%
  mutate(pred = factor(pred, order = TRUE, 
                           levels = c("Strongly Disagree", "Disagree", "Nor", "Agree", "Strongly Agree")))
```  

## Are there some stimuli that got a wide range or responses?
```{r}
believe_data_with_pred %>%
  group_by(image_new, response) %>%
  summarise(n= n()) %>%
  arrange(response) %>%
  pivot_wider(names_from = response, values_from = n, values_fill = 0) %>%
  rowwise() %>%
  mutate(total_uses = sum(`Strongly Disagree`, `Disagree`, `Nor`, `Agree`, `Strongly Agree`)) %>%
  filter(total_uses > 1) %>%
  arrange(desc(total_uses))
```

## Images with largest errors
```{r}
believe_data_with_pred %>%
  group_by(image_new, response) %>%
  summarise(n= n(),
            pred = pred) %>%
  mutate(error = as.numeric(response) - as.numeric(pred)) %>%
  filter(abs(error) > 2) %>%
  arrange(image_new, desc(n))
```
We notice from the collage that most of the mislabeled visualizations are horizontal bar charts, tables, and infographics. 

# Anaylyze trust for every dimension: 

After analyzing trust responses for individuals and images for all dimensions, we build decision trees to see the main predictors for each dimension of trust. For that we filter by extreme agreement and disagreement responses and trust dimension. Given the substantial skew towards strong agreement in responses, we deliberately chose a sample of 125 images from each agreement level to balance our dataset. In order to reduce run time we discard irrelevant columns (that are not related to image attributes or individual characteristics). We then run our decision tree based on the wrangled dataset.

## Belief:

```{r}
strongly_believe_data <- trust_2023_raw_data %>%
  filter(name == "I believe the visualization shows real data." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
strong_belief_prelim_tree <- rpart(response ~.-image_new , data = strongly_believe_data, method="class")
```

```{r}
rpart.plot(strong_belief_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

We find that the significant determinants of credibility are: vis type, colorfulness, and education level

##Clarity:

```{r}
strongly_clear_data <- trust_2023_raw_data %>%
  filter(name == "I understand what this visualization is trying to tell me." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
strong_clarity_prelim_tree <- rpart(response ~.-image_new , data = strongly_clear_data, method="class")
```

```{r}
rpart.plot(strong_clarity_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

The significant determinants of clarity are data literacy, human recognizable objects, and vis type

##Reliability:

```{r}
strongly_reliable_data <- trust_2023_raw_data %>%
  filter(name == "I would rely on the facts in this Visualization." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source)) 
```

```{r}
strong_reliability_prelim_tree <- rpart(response ~.-image_new , data = strongly_reliable_data, method="class")
```

```{r}
rpart.plot(strong_reliability_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

The significant determinants of reliability are vis type and age

##Familiarity

```{r}
strongly_familiar_data <- trust_2023_raw_data %>%
  filter(name == "I am familiar with the topic or data this visualization presents." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale)%>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```

```{r}
strong_familiar_prelim_tree <- rpart(response ~.-image_new , data = strongly_familiar_data, method="class")
```

```{r}
rpart.plot(strong_familiar_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

The significant determinants of familiarity are age and data literacy

##Confidence:

```{r}
strong_confidence_data <- trust_2023_raw_data %>%
  filter(name == "I would feel confident using the information to make a decision." & (response == "Strongly Agree" | response == "Strongly Disagree")) %>%
  group_by(likertscale) %>%
  sample_n(125) %>%
  ungroup() %>%
  select(-c(...1, name, session_id, image, subfolder, category, likertscale, time,
            starts_with("title"), starts_with("mem"), question_type, cluster, source))
```


```{r}
strong_confidence_prelim_tree <- rpart(response ~.-image_new , data = strong_confidence_data, method="class")
```

```{r}
rpart.plot(strong_confidence_prelim_tree, type = 4, clip.right.labs = FALSE, extra=2)
```

The significant determinants of confidence are vis type and age.

From analyzing each of the 5 dimensions of trust's decision trees we conclude that the most common determinants of trust in data visualizations are:

- Visualization type: people tend to trust bars and circles more than other types
- Data Literacy: people with higher data literacy usually tend to trust visualizations more
- Colorfulness: this variable fluctuates but usually more colorful visualizations are more trustworthy
- Education: higher education is usually correlated with higher trust


what happens when we have a combination of indiv diff and vis attributes who wins between the two
try to select more and find if we're overfitting. 

after each tree make a random forest to confirm. make note if there is anything out of ordinary between trees and random forests. 